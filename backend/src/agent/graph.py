import json
from typing import Annotated, TypedDict, List, Sequence
from langchain_core.tools import tool
from langchain_core.messages import BaseMessage
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from langchain_core.messages import ToolMessage
from langchain_core.runnables import RunnableConfig
from langchain_ollama import ChatOllama
from langchain_tavily import TavilySearch


from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode

from agent.langsearch_tool import langsearch_websearch_tool

from dotenv import load_dotenv

load_dotenv()

class AgentState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], add_messages]


##########
## 1. Query Generation
### ç”¨æˆ·ä½¿ç”¨è‡ªç„¶è¯­è¨€è¾“å…¥ï¼Œæ ¹æ®è¾“å…¥ï¼Œç”Ÿæˆæœç´¢æŸ¥è¯¢å…³é”®å­—
def query_generation_node(state: AgentState) -> AgentState:
    system_prompt = """
    ä½ æ˜¯ä¸€ä¸ªæœç´¢æŸ¥è¯¢ç”ŸæˆåŠ©æ‰‹ï¼Œæ ¹æ®ç”¨æˆ·è¾“å…¥ï¼Œç”Ÿæˆç”¨äºæœç´¢å¼•æ“ä½¿ç”¨çš„æœç´¢æŸ¥è¯¢å…³é”®å­—ï¼Œå…³é”®å­—å¯ä»¥æœ‰å¤šä¸ªï¼Œç¡®ä¿èƒ½å¤Ÿæœé€Ÿåˆ°ç”¨æˆ·å…³å¿ƒçš„è¯é¢˜ã€‚
    ç”¨æˆ·ä½¿ç”¨ä¸­æ–‡è¾“å…¥ï¼Œæå–å‡ºçš„æœç´¢å…³é”®å­—ä¸ºè‹±æ–‡ï¼Œå› ä¸ºæŠ€æœ¯æ–‡æ¡£æ˜¯è‹±æ–‡çš„ã€‚å¦‚æœç”¨æˆ·ä¸­è‹±æ–‡æ··åˆè¾“å…¥ï¼Œåˆ™è‹±æ–‡éƒ¨åˆ†æ˜¯å…³é”®å­—çš„ä¸€éƒ¨åˆ†
    æœç´¢èŒƒå›´é™å®šä¸º site:support.apple.com
    
    ä¸¾ä¾‹å¦‚ä¸‹ï¼š
    User: è‹¹æœæ‰‹æœºæ€ä¹ˆè¿æ¥è“ç‰™è€³æœºï¼Ÿ
    Assistant: iPhone Bluetooth connection guide site:support.apple.com

    User: ä»€ä¹ˆæ˜¯Apple Business Managerï¼Ÿ
    Assistant: Apple Business Manager site:support.apple.com
    """
    # llm = ChatOllama(model="Qwen2.5-14B-Instruct:latest")
    llm = ChatOllama(model="qwen2.5:3b")
    messages = [SystemMessage(content=system_prompt)] + state["messages"]
    response = llm.invoke(messages)
    print(f"ğŸ” ç”Ÿæˆæœç´¢æŸ¥è¯¢: {response.content}")
    return {"messages": [response]}
#########



@tool
def search_web(query: str) -> str:
    """Search the web for information with Tavily."""
    search = TavilySearch(max_results=3, include_raw_content=True)
    return search.run(query) 



tools = [langsearch_websearch_tool]
llm = ChatOllama(model="qwen2.5:7b")
llm_with_tools = llm.bind_tools(tools)


def agent_node(state: AgentState) -> AgentState:
    system_prompt = """
    ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„Appleäº§å“æŠ€æœ¯ä¸“å®¶ï¼ŒæœåŠ¡äºä¸­å›½å®¢æˆ·, éœ€è¦å°†è¾“å…¥çš„è‹±æ–‡æ–‡æ¡£ï¼Œæ•´ç†æˆä¸­æ–‡å›ç­”ï¼Œå¹¶ç»™å‡ºå‚è€ƒé“¾æ¥ã€‚
    å¿…é¡»ä½¿ç”¨ä¸­æ–‡æ¥å›ç­”ï¼Œå¦åˆ™ç”¨æˆ·ä¼šç»™å·®è¯„ï¼Œä»è€Œå¯¼è‡´ä½ çš„å·¥ä½œè¢«å–æ¶ˆã€‚
    """
    web_search_result = state["messages"][-1].content
    messages = [SystemMessage(content=system_prompt), HumanMessage(content=web_search_result)]
    response = llm.invoke(messages)
    return {"messages": [response]}



# def agent_node(state: AgentState) -> AgentState:
#     system_prompt = """
#     ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„Appleäº§å“æŠ€æœ¯ä¸“å®¶ï¼ŒæœåŠ¡äºä¸­å›½å®¢æˆ·ã€‚
#     ä½ å¿…é¡»ä½¿ç”¨å·¥å…·æ¥æœç´¢ç›¸å…³ä¿¡æ¯ï¼Œä¸è¦ä¾èµ–è‡ªå·±çš„çŸ¥è¯†ã€‚
#     å¿…é¡»ä½¿ç”¨ä¸­æ–‡æ¥å›ç­”ï¼Œå¦åˆ™ç”¨æˆ·ä¼šç»™å·®è¯„ï¼Œä»è€Œå¯¼è‡´ä½ çš„å·¥ä½œè¢«å–æ¶ˆã€‚
#     """

#     # è·å–æœç´¢æŸ¥è¯¢ï¼ˆæ¥è‡ªquery_generation_nodeï¼‰
#     search_query = state["messages"][-1].content
#     search_instruction = f"å¿…é¡»ç«‹å³ä½¿ç”¨å·¥å…·æœç´¢ï¼š{search_query}"
#     messages = [
#         SystemMessage(content=system_prompt),
#         HumanMessage(content=search_instruction)
#     ]
#     response = llm_with_tools.invoke(messages)
    
#     return {"messages": [response]}

# tools_by_name = {tool.name: tool for tool in tools}

# Define our tool node
# def tool_node(state: AgentState):
#     print("ğŸ”§ æ­£åœ¨è°ƒç”¨æœç´¢å·¥å…·...")
#     outputs = []
#     for tool_call in state["messages"][-1].tool_calls:
#         print(f"ğŸŒ æœç´¢æŸ¥è¯¢: {tool_call['args']['query']}")
#         tool_result = tools_by_name[tool_call["name"]].invoke(tool_call["args"])
#         outputs.append(
#             ToolMessage(
#                 content=json.dumps(tool_result),
#                 name=tool_call["name"],
#                 tool_call_id=tool_call["id"],
#             )
#         )
#     print("âœ… æœç´¢å®Œæˆï¼Œæ­£åœ¨ç”Ÿæˆå›ç­”...")
#     return {"messages": outputs}

def tool_node(state: AgentState):
    print("ğŸ”§ æ­£åœ¨è°ƒç”¨æœç´¢å·¥å…·...")
    outputs = []

    query = state["messages"][-1].content
    print(f"ğŸŒ æœç´¢æŸ¥è¯¢: {query}")
    tool_result = langsearch_websearch_tool.invoke(query, count=1)

    outputs.append(
        ToolMessage(
            content=json.dumps(tool_result),
            name=langsearch_websearch_tool.name,
           tool_call_id="tool-1",
        )
    )
    print("âœ… æœç´¢å®Œæˆï¼Œæ­£åœ¨ç”Ÿæˆå›ç­”...")
    return {"messages": outputs}

# Define the conditional edge that determines whether to call tools or end
def should_continue(state: AgentState):
    messages = state["messages"]
    last_message = messages[-1]
    # If there are tool calls, go to tool node
    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:
        return "tools"
    # Otherwise, we finish
    else:
        return "end"

# langchain_llm = ChatOllama(model="qwen2.5:1.5b")


# Build the graph
graph = StateGraph(AgentState)

# # Add nodes
# graph.add_node("query_generation", query_generation_node)
# graph.add_node("agent", agent_node)
# graph.add_node("tools", tool_node)

# # Add edges
# graph.add_edge(START, "query_generation")
# graph.add_edge("query_generation", "agent")
# graph.add_conditional_edges(
#     "agent",
#     should_continue,
#     {"tools": "tools", "end": END},
# )
# # Add edge from tools back to agent
# graph.add_edge("tools", "agent")

graph.add_node("query_generation", query_generation_node)
graph.add_node("agent", agent_node)
graph.add_node("tools", tool_node)
graph.add_edge(START, "query_generation")
# graph.add_edge("query_generation", "agent")
# graph.add_conditional_edges(
#     "agent", should_continue, {"tools": "tools", "end": END})
graph.add_edge("query_generation", "tools")
graph.add_edge("tools", "agent")
graph.add_edge("agent", END)

app = graph.compile()

# é…ç½®é€’å½’é™åˆ¶
config = {"recursion_limit": 10}

# Helper function for formatting the stream nicely
def print_stream(stream):
    for s in stream:
        message = s["messages"][-1]
        if isinstance(message, tuple):
            print(message)
        else:
            message.pretty_print()



def run_agent():
    print("\n ===== Langgraph Agent =====\n")

    while True:
        user_input = input("User: ")
        if user_input.lower() in ["exit", "quit", "q"]:
            break
        messages = [HumanMessage(content=user_input)]
        result = app.invoke({"messages": messages}, config=config)

        print("\n===== Assistant =====\n")
        print(result["messages"][-1].content)


if __name__ == "__main__":
    run_agent()
